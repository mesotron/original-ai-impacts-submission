<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_yjmssbff5a0t-3{list-style-type:none}ul.lst-kix_yjmssbff5a0t-4{list-style-type:none}ul.lst-kix_yjmssbff5a0t-5{list-style-type:none}.lst-kix_yjmssbff5a0t-4>li:before{content:"\0025cb   "}.lst-kix_yjmssbff5a0t-6>li:before{content:"\0025cf   "}ul.lst-kix_yjmssbff5a0t-6{list-style-type:none}ul.lst-kix_yjmssbff5a0t-7{list-style-type:none}ul.lst-kix_yjmssbff5a0t-8{list-style-type:none}.lst-kix_yjmssbff5a0t-1>li:before{content:"\0025cb   "}.lst-kix_yjmssbff5a0t-5>li:before{content:"\0025a0   "}.lst-kix_yjmssbff5a0t-2>li:before{content:"\0025a0   "}ul.lst-kix_yjmssbff5a0t-0{list-style-type:none}ul.lst-kix_yjmssbff5a0t-1{list-style-type:none}ul.lst-kix_yjmssbff5a0t-2{list-style-type:none}.lst-kix_yjmssbff5a0t-3>li:before{content:"\0025cf   "}.lst-kix_yjmssbff5a0t-8>li:before{content:"\0025a0   "}.lst-kix_yjmssbff5a0t-7>li:before{content:"\0025cb   "}ul.lst-kix_zg1itervbpg1-8{list-style-type:none}ul.lst-kix_zg1itervbpg1-7{list-style-type:none}.lst-kix_zg1itervbpg1-0>li:before{content:"\002756   "}ul.lst-kix_zg1itervbpg1-0{list-style-type:none}.lst-kix_zg1itervbpg1-1>li:before{content:"\0027a2   "}.lst-kix_zg1itervbpg1-2>li:before{content:"\0025a0   "}ul.lst-kix_zg1itervbpg1-2{list-style-type:none}ul.lst-kix_zg1itervbpg1-1{list-style-type:none}ul.lst-kix_zg1itervbpg1-4{list-style-type:none}ul.lst-kix_zg1itervbpg1-3{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_zg1itervbpg1-6{list-style-type:none}ul.lst-kix_zg1itervbpg1-5{list-style-type:none}.lst-kix_zg1itervbpg1-5>li:before{content:"\0027a2   "}.lst-kix_zg1itervbpg1-6>li:before{content:"\0025a0   "}.lst-kix_zg1itervbpg1-3>li:before{content:"\0025cf   "}.lst-kix_zg1itervbpg1-4>li:before{content:"\0025c6   "}.lst-kix_zg1itervbpg1-7>li:before{content:"\0025cf   "}.lst-kix_zg1itervbpg1-8>li:before{content:"\0025c6   "}.lst-kix_yjmssbff5a0t-0>li:before{content:"\0025cf   "}ol{margin:0;padding:0}table td,table th{padding:0}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:italic}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c3{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c9{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c14{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c5{color:inherit;text-decoration:inherit}.c11{padding:0;margin:0}.c7{font-size:15pt;font-style:italic}.c4{margin-left:36pt;padding-left:0pt}.c8{height:11pt}.c2{font-weight:700}.c15{font-style:italic}.c10{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c14 doc-content"><p class="c0"><span class="c7">This is a copy of Gabriel Recchia&rsquo;s original submission to the AI Impacts Essay Competition on the Automation of Wisdom and Philosophy, preserved for reference. I suggest ignoring this version and instead reading the </span><span class="c9 c7"><a class="c5" href="https://www.google.com/url?q=https://thediscontinuity.substack.com/p/should-we-just-be-building-more-datasets&amp;sa=D&amp;source=editors&amp;ust=1728551740772599&amp;usg=AOvVaw3NZAgmTHyv4v3DQp3Ti0Iu">revised version</a></span><span class="c7 c13">, which has been improved following reader feedback.</span></p><p class="c0 c8"><span class="c12"></span></p><p class="c0 c8"><span class="c2 c3"></span></p><p class="c0"><span class="c3 c2">Should we just be building more datasets?</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span>It has been a good five years for the </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=http://www.incompleteideas.net/IncIdeas/BitterLesson.html&amp;sa=D&amp;source=editors&amp;ust=1728551740773148&amp;usg=AOvVaw3-ezSlTK3GQ9QlJF_p-gik">Bitter Lesson</a></span><span class="c1">. General methods leveraging computation have largely swept away the domain-specific hacks and tricks that defined many AI subfields for decades. Do any good counterarguments or caveats still stand? I&rsquo;ll call my favorite &ldquo;The Engineer&rsquo;s Consolation&rdquo;:</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0 c10"><span class="c6">For most tasks, there exists some combination of general methods and domain-specific hacks that outperforms general methods alone&mdash;at least for a while</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span>Michael Nielsen offers a </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://cognitivemedium.com/bitter-lesson&amp;sa=D&amp;source=editors&amp;ust=1728551740773552&amp;usg=AOvVaw34Vqtk0PPdhk9iE8b02Fy1">version</a></span><span class="c1">&nbsp;of this argument, noting that Deep Blue&mdash;which the original Bitter Lesson essay pointed to as an example of the power of general methods&mdash;actually incorporated extensive feature engineering. Deep Blue&rsquo;s hybrid approach didn&rsquo;t stand the test of time, but there was a period of time when integrating a large number of expert human heuristics with a powerful general system was the best strategy for achieving the highest chess performance (at least if one wanted to achieve this within a short timeframe).</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span class="c1">This is cold comfort for researchers invested in specialized, domain-specific algorithms: ultimately, the general approach wins. You could also say it&rsquo;s obvious in an uninteresting way: we shouldn&rsquo;t be surprised if there are usually tweaks around the edges that improve performance on a specific task while leaving the bulk of the heavy lifting to general methods. This is especially true if we consider fine-tuning or in-context learning on a nontrivial number of examples to count. It may feel strange to call fine-tuning on a domain-specific corpus a &ldquo;domain-specific hack&rdquo; &mdash; after all, it works across many domains, as long as you change the corpus. But the point is that there&#39;s usually a way to perform extra work to optimize a general method for a particular task or domain.</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span class="c3 c2">Tasks Where Fine-Tuning or In-Context Learning Probably Helps</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span class="c1">What are some tasks where general methods are likely to struggle, but are also likely to benefit from fine-tuning or prompting with an appropriate task-specific corpus? Strong contenders include any task requiring a deep or expert-level underlying conceptual model of a specific domain, where the domain is complex relative to the amount of information about it in the training data, but bounded enough that a useful task-specific corpus for fine-tuning or prompting can actually be constructed. Examples include:</span></p><p class="c0 c8"><span class="c1"></span></p><ul class="c11 lst-kix_yjmssbff5a0t-0 start"><li class="c0 c4 li-bullet-0"><span class="c2">Writing reinforcement learning pipelines</span><span class="c1">&nbsp;for training simulated quadrupeds to perform navigation tasks in simulated environments</span></li><li class="c0 c4 li-bullet-0"><span class="c2">Constructing statistical models</span><span class="c1">&nbsp;to predict the impact of climate change on rainfall within specific ecosystems</span></li><li class="c0 c4 li-bullet-0"><span class="c2">Generating specific testable hypotheses</span><span class="c1">&nbsp;that could help resolve controversies about the nature of human memory, that cognitive psychologists with expertise in human memory would consider worth testing</span></li><li class="c0 c4 li-bullet-0"><span class="c2">Creating detailed logistical plans</span><span class="c1">&nbsp;for game studios to produce games with specific parameters (feature set, team size, technology stack) within a given timeline</span></li></ul><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span class="c1">I&rsquo;ve purposefully limited each of these skills to fairly narrow domains. With adequate time and effort, it&#39;s plausible that LLM-knowledgeable specialists could curate a set of examples and field-specific information to substantially enhance GPT-4 or Claude Sonnet&#39;s capabilities in these areas through targeted fine-tuning and prompting. While it might not be worth the effort to enhance performance in such narrow domains, and the models might not achieve human expert levels, it would be surprising if they couldn&#39;t substantially outperform a na&iuml;ve zero-shot prompt.<br></span></p><p class="c0"><span class="c1">These domains are sufficiently constrained that assembling an effective task-specific dataset for fine-tuning or prompting is feasible. What if we are interested in more general skills?<br></span></p><p class="c0"><span class="c2">Tasks Where It Would Be Good to Know Whether Fine-Tuning or In-Context Actually Helps</span><span class="c1"><br></span></p><p class="c0"><span>To foster &ldquo;wisdom&rdquo; and avoid large-scale errors in general, it&rsquo;s worth considering whether this approach would generalize to relevant meta-skills. For example, we just considered the example of a system provided with a large amount of information in its context window &amp; fine-tuning corpus which was specifically tailored to improve its ability to generate hypotheses that experts in human memory would consider worth testing. If we broadened the input to do the same for a range of other topics within other scientific fields, how good would the resulting system be at experimental design and hypothesis generation </span><span class="c15">in general</span><span class="c1">? Is there a latent skill of &ldquo;stepping back and looking for inconsistencies in your reasoning&rdquo; that can be reliably elicited by providing enough examples of doing this well across several different domains? These kinds of questions seem to me to be both unanswered and empirically answerable. A few ideas for further exploration:</span></p><p class="c0 c8"><span class="c1"></span></p><ul class="c11 lst-kix_zg1itervbpg1-0 start"><li class="c0 c4 li-bullet-0"><span>Scalable oversight strategies such as </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/abs/1805.00899&amp;sa=D&amp;source=editors&amp;ust=1728551740775334&amp;usg=AOvVaw1EfnOSkG-O9OUumE7w-SRF">debate</a></span><span>&nbsp;and </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/abs/2206.05802&amp;sa=D&amp;source=editors&amp;ust=1728551740775556&amp;usg=AOvVaw3w-MpyYHtVzpZsIIGvpGCd">critiquing</a></span><span>&nbsp;require models to be really good at </span><span class="c2">identifying problems with arguments</span><span>, particularly in cases where errors might be subtle or misleading. Identifying problems with arguments also seems like the sort of thing we would like models to do well if we are to identify issues with plans, recommendations, or other model outputs that initially appear reasonable but could have deeper flaws. Anecdotally, my experience suggests that GPT-4 and Claude Sonnet are only so-so at correctly identifying issues with their own outputs, and are sometimes surprisingly poor even when given substantial hints. Is this something that could be improved on given the right kind of prompting or fine-tuning? One could imagine pulling together a large number of datasets illustrating successful error identification across a wide range of domains, using this to curate a well-designed fine-tuning corpus or multi-shot prompt, and testing whether this elicits improvements in error detection ability that generalizes to domains not represented in the prompt or fine-tuning corpus. Some potentially useful datasets in this regard could be </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/abs/2206.05802&amp;sa=D&amp;source=editors&amp;ust=1728551740776062&amp;usg=AOvVaw2QDWnKHGKqeV9YoiZoULz8">Saunders et al. 2022</a></span><span>, </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/abs/2305.20050&amp;sa=D&amp;source=editors&amp;ust=1728551740776274&amp;usg=AOvVaw2u21JGZIYrjDuXZXnMVE6v">Lightman et al. 2023 (PRM800K)</a></span><span>, </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/pdf/2407.04622&amp;sa=D&amp;source=editors&amp;ust=1728551740776501&amp;usg=AOvVaw1CI1J_PPcP2VyVwPGFRyes">Kenton et al. (2024)&rsquo;s modification</a></span><span>&nbsp;of </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/abs/2210.01240&amp;sa=D&amp;source=editors&amp;ust=1728551740776682&amp;usg=AOvVaw0v9zPWSJl28OTFhnvpSJVE">PrOntoQA</a></span><span>, bug detection benchmarks such as </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/9712116&amp;sa=D&amp;source=editors&amp;ust=1728551740776933&amp;usg=AOvVaw3vqRhQQY1f1O27P3Qi4-Xx">PyTraceBugs</a></span><span>&nbsp;and </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0164121220301436&amp;sa=D&amp;source=editors&amp;ust=1728551740777179&amp;usg=AOvVaw3DtLDvaO7pZ3PCKx3seYTv">BugHunter</a></span><span>, and hallucination benchmarks such as </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/pdf/2310.00259&amp;sa=D&amp;source=editors&amp;ust=1728551740777400&amp;usg=AOvVaw2-AAxLv8cg5dKBoYiD8j9n">Autohall</a></span><span>, </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/pdf/2305.11747&amp;sa=D&amp;source=editors&amp;ust=1728551740777587&amp;usg=AOvVaw1d54fAiE6u7MWXW_qEDSHP">HaluEval</a></span><span>, </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://arxiv.org/abs/2307.15343&amp;sa=D&amp;source=editors&amp;ust=1728551740777765&amp;usg=AOvVaw2NqD-jKxr_W3ECHvxiNeI4">Med-HALT</a></span><span>, and </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://aclanthology.org/2023.emnlp-main.42/&amp;sa=D&amp;source=editors&amp;ust=1728551740777962&amp;usg=AOvVaw1zva7c6etXAsZJRNwBe8WL">HalOmi</a></span><span class="c1">. Of these, only PRM800K and Kenton et al.&rsquo;s modification of PrOntoQA contain annotations of errors in claims that proceed from premises to conclusion&mdash;the key error type that we would like debate models to detect&mdash;and are limited to mathematics and logic respectively. Pulling together relevant datasets from the literature that could be used to train and evaluate models in a wide range of error detection contexts seems like a worthwhile project. For example, it may turn out that a model is very good at identifying errors in formal domains like mathematics and logic, but that this skill does not generalize well to hard problems in less structured domains like philosophy or economics. If so, this would be really important to know before we put too much stock in their ability to self-critique on topics that are beyond our ability to evaluate well. Novel datasets in which subtle errors in step-by-step arguments have been annotated across a variety of domains and difficulty levels could also enable researchers to more fully explore model strengths and limitations in this regard.</span></li></ul><p class="c0 c8 c10"><span class="c1"></span></p><ul class="c11 lst-kix_zg1itervbpg1-0"><li class="c0 c4 li-bullet-0"><span>Some oversight strategies also presume that we can do a good job at getting models to </span><span class="c2">factor complex tasks</span><span class="c1">&nbsp;into a hierarchy of mostly independent subtasks. Can we develop evaluations to assess a model&rsquo;s ability to do this across a wide range of task types and domains? <br><br>Because there are potentially many valid ways to decompose many kinds of tasks, evaluation seems likely to be slow and expensive (e.g., human domain experts evaluating proposed plans / decompositions), but there may be clever ways to get good proxy metrics. For example, domain experts could perform a one-time comparison of their own decompositions of a complex problem in their field to those generated by GPT-4, and use what they learned from this process to enumerate characteristics of successful decompositions, creating a rubric. A judge LLM could then use this rubric to evaluate decompositions generated by future, more capable LLMs. </span></li></ul><p class="c0 c8 c10"><span class="c1"></span></p><ul class="c11 lst-kix_zg1itervbpg1-0"><li class="c0 c4 li-bullet-0"><span>A classic example of &ldquo;smart but not wise&rdquo; is when subtle, hard-to-detect errors add up, leading to incorrect or even absurd conclusions. Examples include the </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=https://www.lesswrong.com/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem&amp;sa=D&amp;source=editors&amp;ust=1728551740778826&amp;usg=AOvVaw2iiPxxbFbSQomDLiLPNClr">obfuscated arguments problem</a></span><span>&nbsp;and my </span><span class="c9"><a class="c5" href="https://www.google.com/url?q=http://www.twonewthings.com/images/google_hypernyms.jpg&amp;sa=D&amp;source=editors&amp;ust=1728551740778993&amp;usg=AOvVaw3AChAXwomtnMe8CJdxLyk2">favorite WordNet hypernym chain</a></span><span>. Being able to </span><span class="c2">recognize when subtle errors have accumulated</span><span class="c1">&nbsp;has occurred seems like an important ability for avoiding errors of this kind. Relatedly, while it makes sense that Lightman et al. 2023 focuses on detecting initial errors in the context of mathematical reasoning, there are domains where this might run into practical difficulties distinguishing between ambiguously stated but fundamentally sound claims and technically correct but misleading statements that set the stage for more serious errors. A language model&rsquo;s ability to identify misleading (but not necessarily explicitly incorrect) statements in arguments could end up being an important skill to train and evaluate. Once again, there is a lot of work to be done to acquire relevant training and evaluation data across a wide range of domains.</span></li></ul><p class="c0 c8"><span class="c1"></span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span class="c1">Building out datasets and evaluations focused on &#39;wisdom-relevant skills&#39; (e.g. error detection, task decomposition, cumulative error recognition, identification of misleading statements, etc.) could significantly contribute to our ability to make progress on scalable oversight and avoid large-scale errors more broadly. This work is relatively accessible, not requiring insider access to unpublished frontier models, specialized ML experience, or extensive resources. In principle, many individuals can contribute to creating datasets and evaluations of this kind.</span></p><p class="c0 c8"><span class="c1"></span></p><p class="c0"><span class="c1">Of course, these skills also represent capabilities that are concerning if misused by systems (or system operators) with goals contrary to our own. This is beyond the scope of this essay, but my current position is that (i) to the extent that models already possess these abilities latently, such that they can be elicited by methods as simple as fine-tuning and prompting, a significant portion of the associated risk is already present; and (ii) the contribution that harnessing these abilities would make to scalable oversight would likely be net-positive, as it would increase the probability that scalable oversight methods can help us navigate a critical risk period in which sufficiently specialized &#39;savant&#39; models&mdash;trained to be superhuman at particular alignment-relevant tasks, without superhuman general intelligence&mdash;actually can &quot;do our alignment homework&quot;. I&rsquo;m uncertain about this position, and can imagine circumstances where it could go wrong. For example, if a fine-tuning corpus for task decomposition were to be developed, and if it turned out that this corpus were to greatly improve an LLM&rsquo;s general planning abilities when fine-tuned upon, and if fine-tuning LLMs on that corpus were to become part of the standard training pipeline for frontier models, then I might worry that this situation is no longer well-described as &ldquo;eliciting a latent capability&rdquo;, and rather could be adding a new capability in a way that could increase risk. However, to the extent that we can make the best use of the wisdom-relevant capabilities latent within existing models with low risk of introducing risky capabilities into future models, I&rsquo;m fairly confident that we should do so. </span></p><p class="c0 c8"><span class="c1"><br></span></p><p class="c0"><span class="c6">Claude Sonnet and GPT-4 assisted with word choice and helped revise for style and readability, and suggested the example of regional climate prediction in the first set of bullet points.</span></p></body></html>
